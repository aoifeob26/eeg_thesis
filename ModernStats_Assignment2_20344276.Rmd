---
title: "ModernStatMethods_Assignment2"
output: html_document
date: "2025-04-03"
author: Alannah Blacoe 20344276
---


**Background**
This project uses EEG data collected from participants who were shown visual stimuli (images) during the experiment. Each trial contains 256 timepoints recorded from multiple scalp electrodes over a one-second window. For this analysis, only the first trial per subject was used, making it a relatively small dataset overall. The goal was to classify subjects into two groups: 'a' for alcoholics and 'c' for controls, based on their brain activity patterns. Specifically, the aim was to see whether tree-based models like Random Forests could accurately distinguish between these groups using only EEG-derived features.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fda)
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(vip)
library(rpart)
library(rpart.plot)
library(partykit)

```

```{r}
# Dataset contains EEG recordings from multiple subjects, with 256 timepoints per trial.
# Only the first trial per subject is used, making this a relatively small dataset overall (originally only using scores FP1 & FP2).


firsttrial <- readRDS("firsttrial_allsensors_data.rds")
time <- seq(0, 1, length.out = 256)

nharm <- 20
basis <- create.fourier.basis(rangeval = range(time), nbasis = nharm*2)
fdParObj <- fdPar(basis, Lfdobj = 2, lambda = 1e-2)

extract_scores <- function(electrode) {
  data <- firsttrial %>% filter(sensor.position == electrode) %>%
    arrange(matching.condition, subject.identifier, name, time)
  mat <- matrix(as.numeric(data$sensor.value), nrow = 256)
  fd <- smooth.basis(time, mat, fdParObj)$fd
  pca <- pca.fd(fd, nharm = nharm)
  scores <- as.data.frame(pca$scores)
  colnames(scores) <- paste0(electrode, "_", 1:nharm)
  return(scores)
}

scores_fp1 <- extract_scores("FP1")
scores_fp2 <- extract_scores("FP2")
scores_f3  <- extract_scores("F3")
scores_f4  <- extract_scores("F4")

meta <- firsttrial %>% filter(sensor.position == "FP1", time == 0) %>%
  dplyr::select(subject.identifier, matching.condition)
meta$subject.identifier <- as.factor(meta$subject.identifier)

# Base model data: FP1 + FP2
fpca_base <- cbind(meta, scores_fp1, scores_fp2)

# Full model data: FP1–F4
fpca_full <- cbind(meta, scores_fp1, scores_fp2, scores_f3, scores_f4)

```

```{r}
# Train a baseline Random Forest model using only FP1 & FP2 electrode scores

features_base <- grep("^(FP1|FP2)_", names(fpca_base), value = TRUE)
model_base <- fpca_base %>% dplyr::select(subject.identifier, all_of(features_base))

set.seed(123)
split_base <- createDataPartition(model_base$subject.identifier, p = 0.7, list = FALSE)
train_base <- model_base[split_base, ]
test_base  <- model_base[-split_base, ]

formula_base <- as.formula(paste("subject.identifier ~", paste(features_base, collapse = " + ")))
rf_base <- randomForest(formula_base, data = train_base, importance = TRUE)
pred_base <- predict(rf_base, newdata = test_base)
print(confusionMatrix(pred_base, test_base$subject.identifier))

```
**Interpretation:**  
Using just the FP1 & FP2 electrodes from my cleaned thesis dataset, the model reached a test accuracy of 42.9%. These electrodes are usually linked to cognitive activity, and therefore made sense as a starting point. Even though the result is below chance (50%), it gives a good baseline to build from.

```{r}

# Tuning with only FP1 & FP2

importance_base <- as.data.frame(rf_base$importance)
top_base_feats <- rownames(importance_base)[order(importance_base$MeanDecreaseGini, decreasing = TRUE)][1:10]

formula_base_reduced <- as.formula(paste("subject.identifier ~", paste(top_base_feats, collapse = " + ")))
rf_tuned_base <- train(
  formula_base_reduced,
  data = train_base,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 10
)

pred_tuned_base <- predict(rf_tuned_base, newdata = test_base)
print(confusionMatrix(pred_tuned_base, test_base$subject.identifier))

```
**Interpretation:**
After tuning the FP1 & FP2 model using the top features and cross-validation, accuracy became even lower at **35.7%**. This shows that tuning alone isn’t enough — the model just didn’t have strong enough signal to make reliable predictions.

*Next move for improving accuracy:*
Add more scores (like from additional electrodes). If those carry meaningful info, the model should pick up better patterns and improve performance.
Below we will add these scores (F3 & F4)

```{r}

# Train a full Random Forest model using FP1–F4 electrode scores

features_full <- grep("^(FP1|FP2|F3|F4)_", names(fpca_full), value = TRUE)
model_full <- fpca_full %>% dplyr::select(subject.identifier, all_of(features_full))

set.seed(123)
split_full <- createDataPartition(model_full$subject.identifier, p = 0.7, list = FALSE)
train_full <- model_full[split_full, ]
test_full  <- model_full[-split_full, ]

formula_full <- as.formula(paste("subject.identifier ~", paste(features_full, collapse = " + ")))
rf_full <- randomForest(formula_full, data = train_full, importance = TRUE)
pred_full <- predict(rf_full, newdata = test_full)
print(confusionMatrix(pred_full, test_full$subject.identifier))

```

**Interpretation:**
By including F3 & F4 scores alongside FP1 & FP2, test accuracy improved to **57.1%**, which is a 14.2% increase from the baseline. This suggests that the extra frontal electrodes added useful information, likely because they're also involved in cognitive, emotional, and attention-related processing much like FP1 & FP2.



```{r}

# Visualise the top 10 most important features driving classification

x_train <- train_full[, features_full]
y_train <- train_full$subject.identifier

rf_temp <- randomForest(x = x_train, y = y_train, importance = TRUE)

importance_df <- as.data.frame(rf_full$importance)
top_features <- rownames(importance_df)[order(importance_df$MeanDecreaseGini, decreasing = TRUE)][1:10]

formula_reduced <- as.formula(paste("subject.identifier ~", paste(top_features, collapse = " + ")))
rf_tuned <- train(
  formula_reduced,
  data = train_full,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 10
)

rf_pred_tuned <- predict(rf_tuned, newdata = test_full)
print(confusionMatrix(rf_pred_tuned, test_full$subject.identifier))

```
**Interpretation:**  
Using the 10 most important features and tuning `mtry` via 5-fold cross-validation improved accuracy to **64.3%**, which is seen to be the most accuarte model so far with an increase of **21.44%**. This supports the idea that feature selection & hyperparameter tuning helps avoid overfitting and improves generalisation.




```{r}

# Visual comparison of accuracy for baseline, full, and tuned models

barplot(
  c(
    FP1_FP2 = mean(pred_base == test_base$subject.identifier),
    All4 = mean(pred_full == test_full$subject.identifier),
    Tuned = mean(rf_pred_tuned == test_full$subject.identifier)
  ),
  names.arg = c("FP1+FP2", "FP1,2 + F3,4", "Top10 Tuned"),
  col = c("skyblue", "violetred", "seagreen3"),
  ylim = c(0, 1),
  main = "Test Set Accuracy by Model",
  ylab = "Accuracy"
)

```

**Interpretation:**  
The barplot clearly shows that the tuned model outperforms the FP1 + FP2 baseline and also the full 4-electrode model. From adding F3/F4 to selecting top features, all steps helped improve performance.


```{r}

# Visualise top 10 most important features from the tuned Random Forest

vip(rf_tuned$finalModel, num_features = 10, geom = "col") +
  ggplot2::aes(fill = Importance) + 
  ggplot2::scale_fill_gradient(low = "yellow2", high = "mediumpurple")


```

**Interpretation:**
The most important features include FP2_12, FP2_9, and FP1_10, suggesting that frontal electrodes (FP1 & FP2) contribute significantly to classification. However, F3_3, F4_12, and various other F3/F4 features also rank highly (5/10 of the top values), validating the decision to include them in the model. This indicates that while FP1 & FP2 remain dominant, additional frontal sensors (F3 & F4) provide useful complementary information.


```{r}

# Build and prune a decision tree model to improve interpretability

tree_model <- rpart(formula_full, data = train_full, method = "class", cp = 0.001)
rpart.plot(tree_model, main = "Full Tree")
printcp(tree_model)
plotcp(tree_model)

opt_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = opt_cp)
rpart.plot(pruned_tree, main = "Pruned Tree")

tree_pred <- predict(pruned_tree, newdata = test_full, type = "class")
print(confusionMatrix(tree_pred, test_full$subject.identifier))

```

**Interpretation:**
Above F4_12 is used as the model has determined it to be an important feature that helps reduce uncertainty and make better predictions.
The decision tree offers a clear, visual way to understand which features such as **F4_12** are important for classification. 
The full tree appears quite complex and likely overfits the training data. Pruning the tree helps simplify the structure, in return making it easier to interpret, however the overall accuracy remains lower than the Random Forest models.
While the decision tree doesn’t perform as well in terms of prediction, it’s still valuable for understanding how the model is making decisions and which variables are driving the splits.


```{r}

# Train and evaluate a ctree model to compare statistical splitting performance


ctree_model <- ctree(formula_full, data = train_full)
plot(ctree_model, main = "Conditional Inference Tree", 
     tp_args = list(fill = c("darkturquoise", "violetred")))


ctree_pred <- predict(ctree_model, newdata = test_full)
print(confusionMatrix(ctree_pred, test_full$subject.identifier))


```

**Interpretation:** 
The ctree model uses statistical tests to decide where to split the data, however appears to struggle with making meaningful divisions in this context. The model didn’t split the data at all and the first node had an even mix of both classes ‘a’ & ‘c’, indicating that the model fails to detect clear patterns in the data and couldn't find a good way to seperate them. This is likely due to the small dataset size and ctree’s stricter statistical criteria for splitting. As a result, the model defaults to predicting the majority class, which leads to 50% accuracy and is no better than random guessing.

This would suggest that the ctree model might not be the best fit for this dataset, as it requires more statistical significance for splits. Simpler trees or more flexible models like Random Forests (as shown above) performed better.



```{r}

# Summarise model performance across all approaches in a comparison table


results_df <- data.frame(
  Model = c("FP1+FP2", "FP1–F4", "Top 10 Tuned"),
  Accuracy = c(
    mean(pred_base == test_base$subject.identifier),
    mean(pred_full == test_full$subject.identifier),
    mean(rf_pred_tuned == test_full$subject.identifier)
  )
)

knitr::kable(results_df, caption = "Comparison of Model Accuracy")

```


**Summary:**
The final results highlight how the Random Forest model with tuned hyperparameters & selected features performed the best overall. While decision trees were useful for interpretation, they didn’t reach the same level of accuracy. When creating a version of the dataset that also included the F3 and F4 electrodes (alongside FP1 & FP2), accuracy improved. As mentioned above, these additional electrodes are associated with emotional & motor processing, which would therefor make sense that they added useful information to help the model distinguish between the two subject groups.

This aligns with what we’ve covered in lectures, that the ensemble models like Random Forests tend to perform better because they combine multiple trees and randomly sample features, which helps avoid overfitting and improves generalisation.

This reinforces the importance of thoughtful feature selection and model tuning, particlularly when working with smaller EEG datasets.


**Conclusion**
In this project, I tested out different tree-based models using EEG data. I started with a simple baseline using just FP1 & FP2 sensors, and then improved the model by adding more electrodes (F3 & F4) and tuning the model. The best accuracy I got was **64.3%**, which came from using the top 10 most important features and tuning the 'mtry' parameter.

Overall, this backs up what we’ve seen in lectures, which is that Random Forests tend to work well because they use many trees and randomly select features, which helps reduce overfitting. Feature engineering was also key here, especially using FPCA and being selective about which electrodes to include.

Although simpler models like ctrees were easy to understand/interpret, they didn’t perform as well. One way to possibly improve accuracy further would be to increase the sample size or collect more trials per participant, as this could help the model pick up more reliable patterns and improve generalisation.


