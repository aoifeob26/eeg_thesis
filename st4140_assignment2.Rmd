---
title: "st4140_assignment2"
author: "Aoife O'Brien 21349803"
output: html_document
date: "2025-04-02"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r message=FALSE, warning=FALSE}
library(fda)
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(skimr)
library(table1)
library(rpart)
library(rpart.plot)
library(partykit)
library(vip)
library(knitr)
```


# Introduction
  This report investigates the classification of EEG responses to stimuli using functional principal component analysis (FPCA) on data from two frontal electrodes (FP1 and FP2). The EEG curves are smoothed using a B-spline basis. PCA scores are then used as features in tree-based classification models to predict the stimulus matching condition.



# Data Preparation

```{r}
firsttrial <- readRDS("firsttrial_data.rds")
time <- seq(0, 1, length.out = 256)

# Create bspline basis
basis <- create.bspline.basis(rangeval = range(time), nbasis = length(time)/2)
fdParObj <- fdPar(basis, Lfdobj = 2, lambda = 1e-2)

# FP1
fp1_data <- firsttrial %>% filter(sensor.position == "FP1") %>%
  arrange(matching.condition, subject.identifier, name, time)
mat_fp1 <- matrix(as.numeric(fp1_data$sensor.value), nrow = 256)
fd_fp1 <- smooth.basis(time, mat_fp1, fdParObj)$fd
res_fp1 <- pca.fd(fd_fp1, nharm = 6)
scores_fp1 <- as.data.frame(res_fp1$scores)
colnames(scores_fp1) <- paste0("FP1_", 1:6)

# FP2
fp2_data <- firsttrial %>% filter(sensor.position == "FP2") %>%
  arrange(matching.condition, subject.identifier, name, time)
mat_fp2 <- matrix(as.numeric(fp2_data$sensor.value), nrow = 256)
fd_fp2 <- smooth.basis(time, mat_fp2, fdParObj)$fd
res_fp2 <- pca.fd(fd_fp2, nharm = 6)
scores_fp2 <- as.data.frame(res_fp2$scores)
colnames(scores_fp2) <- paste0("FP2_", 1:6)

# Combine with metadata
meta <- fp1_data %>% filter(time == 0) %>%
  dplyr::select(matching.condition, subject.identifier)
fpca_data <- cbind(meta, scores_fp1, scores_fp2)
fpca_data$matching.condition <- as.factor(fpca_data$matching.condition)
```


A lot of trial and error done with the smoothing parameters to find the optimal set.

Found that with the given smoothing parameters, nharm=7, lamba= 1e-2, lfdobj=2 nbasis= legnth(time)/2, I got the best results for the classification models with my data set and models.


# Summary Statistics

```{r}
skim(fpca_data)
table1(~ ., data = fpca_data)

```








The dataset used for this analysis contains 48 observations and 14 variables. Each row represents an EEG trial from one of 16 participants (8 alcoholic, 8 control), in response to one of three stimulus types:
  S1 obj
  S2 match
  S2 nomatch

There are three trials per participant, one for each condition.

The columns are structured as follows:
  matching.condition: the type of stimulus presented (target variable)
  subject.identifier: the participant group (a = alcoholic, c = control)
  FP1_1 to FP1_6: the first 6 functional principal component (FPC) scores from electrode FP1
  FP2_1 to FP2_6: the first 6 FPC scores from electrode FP2

All numeric columns are complete, and the features have varying distributions, as expected from PCA scores. For example, FP1_1 ranges from -17.3 to 20.9, while FP2_6 is more tightly clustered, ranging from -1.59 to 1.45. This suggests that a few components may capture much of the meaningful variation, which aligns with their importance in tree-based models later.


### Train/test split

```{r}
set.seed(123)
train_idx <- createDataPartition(fpca_data$matching.condition, p = 0.7, list = FALSE)
train_data <- fpca_data[train_idx, ]
test_data <- fpca_data[-train_idx, ]

```


To evaluate model performance fairly, the dataset was split into training and testing subsets using a 70:30 ratio.This strategy helps avoid overfitting by ensuring that models are assessed on data they haven’t seen during training.

### Classification Tree

```{r}
# Fit and plot tree
tree_model <- rpart(matching.condition ~ ., data = train_data, method = "class")
rpart.plot(tree_model, box.palette = "Blues", main = "Classification Tree")

# Predict
tree_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(tree_pred, test_data$matching.condition)

```



### Pruned Tree

```{r}
# Prune using 1-SE rule
printcp(tree_model)
plotcp(tree_model)
opt_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = opt_cp)
rpart.plot(pruned_tree, box.palette = "Blues", main = "Pruned Tree")

```



The classification tree model used the top FPCA scores to split the data based on matching.condition. The pruned version of the tree, chosen using the 1-SE rule included two key features: FP2_6 and FP1_1.
  The first split occurred at FP2_6 >= -0.48, dividing the observations based on this principal component score from the FP2 electrode.
  If FP2_6 >= -0.48, the next split used FP1_1 >= 4 to further classify the trial.

These splits led to terminal nodes with differing proportions of the stimulus types, indicating that the FPCA scores do capture useful differences between conditions.

The overall accuracy was 50%, and Kappa = 0.25, suggesting the model performs better than random guessing at 33% but still leaves room for improvement.

This tree provides high interpretability, as it clearly shows which variables drive the classification and how decisions are made at each split. For example, high values of FP1_1 tend to be associated with S1 obj, while lower scores shift prediction toward S2 nomatch.


### Condition Inference Tree

```{r}
# Fit the default conditional inference tree
ctree_model <- ctree(matching.condition ~ ., data = train_data)
plot(ctree_model, main = "Conditional Inference Tree")

# Predict + evaluate
ctree_pred <- predict(ctree_model, newdata = test_data)
ctree_cm <- confusionMatrix(ctree_pred, test_data$matching.condition)
ctree_cm

```

The conditional inference tree model selects splits based on statistical significance testing rather than greedy impurity reduction, making it more conservative, especially with small datasets.

In this case, the model failed to produce any splits. It predicted all observations in the test set as the most frequent class, resulting in a flat, null tree.

Accuracy was 33.3%, which matches the base rate for a random guess among 3 equal classes.

Kappa was 0.00, confirming that the model did not improve upon random chance.
There is a suggestion the model is highly sensitive to train/test split variation, likely due to the small sample size (n = 48) and modest signal in the predictors.

This conservative behaviour is a known trait of ctree—it uses permutation-based p-values to decide splits, therefore if nothing passes the significance threshold, no split is made.


### Random Forest

```{r}
# Basic Random Forest
rf_model <- randomForest(matching.condition ~ ., data = train_data, importance = TRUE)
rf_pred <- predict(rf_model, newdata = test_data)

# Accuracy + Confusion Matrix
rf_cm <- confusionMatrix(rf_pred, test_data$matching.condition)
rf_cm

# Show numeric accuracy separately
print(paste("Random Forest Accuracy:", round(rf_cm$overall["Accuracy"] * 100, 2), "%"))
print(paste("Kappa:", round(rf_cm$overall["Kappa"], 2)))

# Variable importance plot
varImpPlot(rf_model)

```


The untuned random forest model was trained using 500 trees and a default value for mtry.

Accuracy is 58.3% which is a noticeable improvement over the baseline of 33.3%.

Kappa is 0.38 which is a moderate level of agreement beyond chance, indicating the model is capturing some structure in the data.

The confusion matrix shows that the model successfully classified a reasonable number of cases across all three matching conditions.

FP2_6, FP1_1, and FP2_1 were among the most important predictors based on both MeanDecreaseGini and MeanDecreaseAccuracy.

This suggests that EEG variation captured by those harmonics are useful features for distinguishing stimulus conditions.




### Tuned Random Forest

```{r}
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(matching.condition ~ ., data = train_data,
                  method = "rf", trControl = control, tuneLength = 5)

# Best parameters
rf_tuned$bestTune

# Evaluate
rf_pred <- predict(rf_tuned, newdata = test_data)
confusionMatrix(rf_pred, test_data$matching.condition)

# Plot model performance
plot(rf_tuned)

# Variable importance (VIP package)
vip(rf_tuned$finalModel, num_features = 10)

```


After tuning the model using 5-fold cross-validation, the optimal number of predictors, mtry = 13, was selected.

Accuracy is 66.7% which is the highest among all models tested, showing the value of hyperparameter tuning.

Kappa is 0.50 which represents moderate to substantial agreement, and a clear jump from the untuned version.

The model made accurate predictions across all three classes, improving detection of the "S2 nomatch," condition, which was hardest to classify in earlier models.

The top features included FP2_6, FP2_5, and FP1_1. This reinforces that later FPCA components from FP2 may be capturing key temporal patterns linked to stimulus differentiation.



### FPCA score by condition

```{r}
ggplot(fpca_data, aes(x = matching.condition, y = FP1_1)) +
  geom_boxplot(fill = "lightpink") +
  labs(title = "Top FPCA Score (FP1_1) by Matching Condition",
       x = "Matching Condition", y = "FP1_1 Score")

```


The boxplot of FP1_1 scores across the three matching.condition categories shows clear variation in the first principal component from the FP1 electrode:

S1 obj trials tend to have higher FP1_1 scores with a wider spread.

S2 match and S2 nomatch appear more tightly clustered with lower median scores.

This suggests that FP1_1 captures a strong source of variation related to stimulus type — particularly distinguishing S1 from the others.

Since FP1_1 was frequently selected as a top feature in the decision tree and random forest models, this visual reinforces its importance. It likely reflects a meaningful EEG signature in the early frontal region that differentiates how participants respond to matching versus non-matching stimuli.


# Model Comparison

```{r}

model_comp <- data.frame(
  Model = c(
    "Pruned Classification Tree (rpart)",
    "Conditional Inference Tree (ctree)",
    "Random Forest (untuned)",
    "Tuned Random Forest (mtry = 13)"
  ),
  Accuracy = c("50%", "33%", "58%", "66.7%"),
  Kappa = c("0.25", "0.00", "0.38", "0.50"),
  `Top Features Used` = c(
    "FP2_6, FP1_1",
    "None (no split)",
    "FP2_6, FP1_1, FP2_1",
    "FP2_6, FP2_5, FP1_1"
  ),
  `Interpretability` = c(
    "High (easy to interpret)",
    "Very low",
    "Medium",
    "Lower (but best performance)"
  )
)

kable(model_comp, caption = "Comparison of Tree-Based Models for EEG Classification")

```

# Findings
Among the four tree-based models tested:
  Tuned Random Forest performed best, with 66.7% accuracy and a Kappa of 0.50, indicating moderate agreement beyond chance. It consistently classified all three matching conditions more effectively than other models.

Untuned Random Forest also performed well (58% accuracy, Kappa = 0.38), but slightly underperformed compared to the tuned version.

  Pruned Classification Tree offered 50% accuracy and high interpretability, with just two variables used in decision-making.

  Conditional Inference Tree failed to split the data and predicted only the most frequent class, resulting in chance-level performance.

Overall, there’s a clear trade-off where random forests provide the best predictive performance, especially when tuned, but are harder to interpret.

Decision trees are easier to explain and visualise, but offer lower accuracy on this dataset.

Across all models, FP1_1 and FP2_6 consistently appeared as important features, highlighting their value in distinguishing EEG responses to different stimuli.



# Conclusion

  This project applied tree-based classification models to EEG data transformed using functional principal component analysis (FPCA). The goal was to predict stimulus matching conditions based on brain responses from frontal electrodes FP1 and FP2.

Four models were compared:
    A pruned classification tree 
    A conditional inference tree 
    An untuned random forest
    A tuned random forest using cross-validation

The tuned random forest achieved the highest performance, making it the most effective model for this task. However, the pruned classifcation tree offered greater interpretability and insight into key features, despite its lower accuracy. The conditional inference model underperformed, failing to make any splits—likely due to the small sample size and its conservative nature.

Across models, FP1_1 and FP2_6 were consistently important predictors. This suggests that early EEG activity in the frontal regions contains useful signals for distinguishing stimulus types. These results are promising but should be interpreted cautiously due to the small dataset. Further work with larger samples and additional electrodes could help validate these findings and improve classification accuracy.

