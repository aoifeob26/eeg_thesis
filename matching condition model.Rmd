---
title: "mathcing condition model"
output: html_document
date: "2025-03-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# Load necessary libraries
library(tidyverse)
library(fda)
library(nnet)
library(caret)
library(glmnet)

# Load dataset
firsttrial <- readRDS("firsttrial_data.rds")
```

```{r}
# Define the time grid for 256 points between 0 and 1
time <- seq(0, 1, length.out = 256)

# Define B-spline basis parameters for smoothing FPCA data
norder <- 10
nbasis <- length(time) + norder - 2
basis <- create.bspline.basis(range(time), nbasis, norder, time)

# Define the penalty and smoothing parameter
Lfdobj <- 4    
lambda <- 1e-2
fdParObj <- fdPar(fd(matrix(0, nbasis, 1), basis), Lfdobj, lambda)

# Smoothing helper function: converts a sensor matrix into a functional data object.
smooth_fd <- function(sensor_matrix, time, fdParObj) {
  smooth.basis(time, sensor_matrix, fdParObj)$fd
}

# Extract FP1 sensor data
fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)



# Convert the FP1 time series data into a matrix form and smooth it to get the functional data object
pca_matrix <- matrix(as.numeric(fp1_data$sensor.value), nrow = 256, ncol = 48)
pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd

# Apply FPCA to extract principal components
res_fp1 <- pca.fd(pca_fd, nharm = nbasis) 

# Determine the number of FPCs that explain at least 95% of the variance
nharms_95 <- which(cumsum(res_fp1$varprop) >= 0.95)[1]

# Re-run FPCA with the determined number of components
res_fp1 <- pca.fd(pca_fd, nharm = nharms_95)
plot(res_fp1)
```



```{r}
# Extract FP1 sensor data for alcoholics ("a")
alc_data <- firsttrial %>%
  filter(sensor.position == "FP1", subject.identifier == "a") %>%
  dplyr::select(name, time, sensor.value)

# Convert sensor values to a matrix (256 time points x 8 trials)
alc_fd_matrix <- matrix(as.numeric(alc_data$sensor.value), nrow = 256, ncol = 8)

# Assuming 'time' is defined and 'fdParObj' is set
# Smooth the matrix to obtain a functional data object
alc_fd <- smooth.basis(time, alc_fd_matrix, fdParObj)$fd

# Extract FP1 sensor data for controls ("c")
control_data <- firsttrial %>%
  filter(sensor.position == "FP1", subject.identifier == "c") %>%
  dplyr::select(name, time, sensor.value)

# Convert sensor values to a matrix (256 time points x 8 trials)
control_fd_matrix <- matrix(as.numeric(control_data$sensor.value), nrow = 256, ncol = 8)

# Smooth the matrix to obtain a functional data object for controls
control_fd <- smooth.basis(time, control_fd_matrix, fdParObj)$fd

# Perform FPCA for alcoholics (FP1 sensor)
res_alc <- pca.fd(alc_fd, nharm = nharms_95)
plot(res_alc)

# Perform FPCA for controls (FP1 sensor)
res_control <- pca.fd(control_fd, nharm = nharms_95)
plot(res_control)


```




```{r}
# Extract FP1 ordering data at time 0 for modeling
fp1_order <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr:: select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  filter(time == 0) %>%
  arrange(matching.condition, subject.identifier, name)

# Extract the full FP1 time series data
fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
 dplyr::  select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

```


```{r}
# Convert FP1 data to a matrix (256 x 48) and smooth it
pca_matrix_fp1 <- matrix(as.numeric(fp1_data$sensor.value), nrow = 256, ncol = 48)
pca_fd_fp1 <- smooth.basis(time, pca_matrix_fp1, fdParObj)$fd

# Run FPCA to extract 5 principal components
resfp1 <- pca.fd(pca_fd_fp1, nharm = nharms_95)
plot(resfp1)

# Extract FPCA scores and prepare modeling data
scores <- data.frame(resfp1$scores)
colnames(scores) <- paste0("FP1.", 1:nharms_95)

# Create a factor for the condition based on matching.condition
fp1_order$condition_factor <- factor(fp1_order$matching.condition)

# Combine ordering information with FPCA scores
fp1modeldata <- cbind(fp1_order, scores)

# Build a multinomial logistic regression model using FP1 scores
model_fp1 <- nnet:: multinom(condition_factor ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5, 
                      data = fp1modeldata)
summary(model_fp1)

```


```{r}
# Extract FP2 data
fp2_data <- firsttrial %>%
  filter(sensor.position == "FP2") %>%
  dplyr:: select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

# Convert FP2 sensor data into a matrix (256 x 48) and smooth it
pca_matrix_fp2 <- matrix(as.numeric(fp2_data$sensor.value), nrow = 256, ncol = 48)
pca_fd_fp2 <- smooth.basis(time, pca_matrix_fp2, fdParObj)$fd

# Run FPCA on FP2 data to extract 5 principal components
resfp2 <- pca.fd(pca_fd_fp2, nharm = nharms_95)
plot(resfp2)

# Extract FP2 scores
scores2 <- data.frame(resfp2$scores)
colnames(scores2) <- paste0("FP2.", 1:nharms_95)

```


```{r}
# Combine FP1 ordering data with FP1 and FP2 FPCA scores
fp1modeldata <- cbind(fp1_order, scores, scores2)

# Build a multinom model predicting subject identifier using combined scores
model_combined <- nnet:: multinom(matching.condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 +
                             FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5, 
                           data = fp1modeldata)
summary(model_combined)

```

```{r}
## Extract FP1 sensor data at time 0, arrange by subject and condition for modeling
 
fp1_order <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  filter(time==0) %>%
  arrange(matching.condition, subject.identifier, name)

fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

## Convert the FP1 time series data into a matrix form and smooth it to get the functional data object
pca_matrix <- matrix(as.numeric(fp1_data$sensor.value), 
                     nrow = 256, ncol = 48)

## Apply FPCA to extract 5 principal components and plot the FPCA results
pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd
resfp1 <- pca.fd(pca_fd, nharm = nharms_95)

plot(resfp1)


scores <- data.frame(resfp1$scores)
colnames(scores) <- c("FP1.1", "FP1.2", "FP1.3", "FP1.4", "FP1.5","FP1.6","FP1.7","FP1.8","FP1.9")
fp1_order$condition_factor <- factor(fp1_order$matching.condition)
#condition <- c(rep("S1 obj", 16), rep("S2 match", 16), rep("S2 No match,", 16))
#condition_factor <- factor(condition, levels = c("S1 obj", "S2 match", "S2 No match,"))
fp1modeldata = cbind(fp1_order,scores)

model <- nnet:: multinom(condition_factor ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9, data = fp1modeldata)


summary(model)
```

Building a Multinomial Model with FP1 and FP2 Data

```{r}
fp2_data <- firsttrial %>%
  filter(sensor.position == "FP2") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

## Convert the FP2 data into a matrix and smooth it to create a functional data object
pca_matrix <- matrix(as.numeric(fp2_data$sensor.value), 
                     nrow = 256, ncol = 48)


pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd
resfp2 <- pca.fd(pca_fd, nharm = nharms_95)

plot(resfp2)


scores2 <- data.frame(resfp2$scores)
colnames(scores2) <- c("FP2.1", "FP2.2", "FP2.3", "FP2.4", "FP2.5","FP2.6","FP2.7","FP2.8","FP2.9")

fp1modeldata = cbind(fp1_order,scores, scores2)

## Build a multinomial logistic regression model to predict subject identifier using combined FP1 & FP2 scores
model <- nnet:: multinom(matching.condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 + FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, data = fp1modeldata)


summary(model)
```

Train-Test Split and Model Evaluation

```{r}

set.seed(123)  
 # clustering functions kmeans etc

## Partition the data into 75% training and 25% testing based on matching.condition
train_indices <- createDataPartition(fp1modeldata$matching.condition, p =0.75, list=F) #matching.condition and subject identifier

as.vector(train_indices)

train_scores <- fp1modeldata[train_indices, ]
test_scores <- fp1modeldata[-train_indices, ]
train_condition <- train_scores$condition_factor
test_condition <- test_scores$condition_factor



train_scores_df <- data.frame(train_scores)
test_scores_df <- data.frame(test_scores)

## Partition the data into 75% training and 25% testing based on matching.condition
train_model_fp1 <- nnet:: multinom(train_condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 , data = train_scores_df)

## Generate predictions on the test set and create a confusion matrix to evaluate performance
predictions <- predict(train_model_fp1, newdata = test_scores_df)


confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)


## Calculate and print the model's prediction accuracy
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)


```

Multinomial Model Using FP2 Scores Only

```{r}

## Train a multinomial logistic regression model using only the FP2 principal component scores.
train_model_fp2 <- nnet:: multinom(train_condition ~ FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9 , data = train_scores_df)


predictions <- predict(train_model_fp2, newdata = test_scores_df)

## Create a confusion matrix to compare predictions to actual test conditions.
confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)

## Calculate and print the accuracy of the FP2 model on the test set.
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)
```

Multinomial Model Using FP1 Scores Only

```{r}
## Partition the data based on subject identifier (via condition_factor) into training and test sets.
train_indices <- createDataPartition(fp1modeldata$matching.condition, p =0.75, list=F) #matching.condition and subject identifier

as.vector(train_indices)

train_scores <- fp1modeldata[train_indices, ]
test_scores <- fp1modeldata[-train_indices, ]
train_condition <- train_scores$condition_factor
test_condition <- test_scores$condition_factor



train_scores_df <- data.frame(train_scores)
test_scores_df <- data.frame(test_scores)

## Train a multinomial logistic regression model using FP1 principal component scores.
train_model_fp1 <- nnet:: multinom(train_condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 , data = train_scores_df)

## Predict matching.conditions on the test dataset using the FP1 model.
predictions <- predict(train_model_fp1, newdata = test_scores_df)

## Build a confusion matrix to compare predicted vs. actual matching.conditions.
confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)

## Calculate and print the model accuracy for the FP1-based predictions
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)

```

accuracy really low above


```{r}
set.seed(123)
train_indices <- createDataPartition(fp1modeldata$subject.identifier, p = 0.75, list = FALSE)
train_data <- fp1modeldata[train_indices, ]
test_data <- fp1modeldata[-train_indices, ]

# Extract the scores and convert them to a matrix
predictors_train <- as.matrix(train_data[, c("FP1.1", "FP1.2", "FP1.3", "FP1.4", "FP1.5", 
                                             "FP1.6", "FP1.7", "FP1.8", "FP1.9", 
                                             "FP2.1", "FP2.2", "FP2.3", "FP2.4", "FP2.5",
                                             "FP2.6", "FP2.7", "FP2.8", "FP2.9")])

predictors_test <- as.matrix(test_data[, c("FP1.1", "FP1.2", "FP1.3", "FP1.4", "FP1.5", 
                                           "FP1.6", "FP1.7", "FP1.8", "FP1.9", 
                                           "FP2.1", "FP2.2", "FP2.3", "FP2.4", "FP2.5",
                                           "FP2.6", "FP2.7", "FP2.8", "FP2.9")])

# Convert the outcome variable into a factor for multinomial regression
response_train <- as.factor(train_data$matching.condition)
response_test <- as.factor(test_data$matching.condition)

# Fit the LASSO model with cross-validation
lasso_model <- cv.glmnet(predictors_train, response_train, alpha = 1, family = "multinomial", type.measure = "class")

# Print the best lambda value
print(paste("Best lambda: ", lasso_model$lambda.min))

# Get the coefficients of the best model
coef(lasso_model, s = "lambda.min")

# Use the fitted LASSO model to make predictions on the test set
predictions_lasso <- predict(lasso_model, newx = predictors_test, s = "lambda.min", type = "class")

# Build confusion matrix to evaluate performance
confusion_matrix_lasso <- table(predictions_lasso, response_test)
print(confusion_matrix_lasso)

# Compute accuracy
accuracy_lasso <- mean(predictions_lasso == response_test)
print(paste("LASSO Model Accuracy:", round(accuracy_lasso * 100, 2), "%"))
```