
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Load necessary libraries
library(tidyverse)
library(fda)
library(nnet)
library(caret)
library(glmnet)

# Load dataset
firsttrial <- readRDS("firsttrial_data.rds")
```

```{r}
# Define the time grid for 256 points between 0 and 1
time <- seq(0, 1, length.out = 256)

# Define B-spline basis parameters for smoothing FPCA data
norder <- 10
nbasis <- norder + length(time) - 2  
basis <- create.bspline.basis(range(time), nbasis, norder, time)

# Define the penalty and smoothing parameter
Lfdobj <- 4    
lambda <- 1e-2
fdParObj <- fdPar(fd(matrix(0, nbasis, 1), basis), Lfdobj, lambda)

# Smoothing helper function: converts a sensor matrix into a functional data object.
smooth_fd <- function(sensor_matrix, time, fdParObj) {
  smooth.basis(time, sensor_matrix, fdParObj)$fd
}

# Extract FP1 sensor data
fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)
```

```{r,eval=F}
# Convert the FP1 time series data into a matrix form and smooth it to get the functional data object
pca_matrix <- matrix(as.numeric(fp1_data$sensor.value), nrow = 256, ncol = 48)
pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd

# Apply FPCA to extract principal components
res_fp1 <- pca.fd(pca_fd, nharm = nbasis)  # Extracting the maximum possible components

# Determine the number of FPCs that explain at least 95% of the variance
nharms_95 <- which(cumsum(res_fp1$varprop) >= 0.95)[1]

# Re-run FPCA with the determined number of components
res_fp1 <- pca.fd(pca_fd, nharm = nharms_95)
plot(res_fp1)
```



```{r, eval=FALSE}
# Extract FP1 sensor data for alcoholics ("a")
alc_data <- firsttrial %>%
  filter(sensor.position == "FP1", subject.identifier == "a") %>%
  dplyr::select(name, time, sensor.value)

# Convert sensor values to a matrix (256 time points x 8 trials)
alc_fd_matrix <- matrix(as.numeric(alc_data$sensor.value), nrow = 256, ncol = 8)

# Assuming 'time' is defined and 'fdParObj' is set
# Smooth the matrix to obtain a functional data object
alc_fd <- smooth.basis(time, alc_fd_matrix, fdParObj)$fd

# Extract FP1 sensor data for controls ("c")
control_data <- firsttrial %>%
  filter(sensor.position == "FP1", subject.identifier == "c") %>%
  dplyr::select(name, time, sensor.value)

# Convert sensor values to a matrix (256 time points x 8 trials)
control_fd_matrix <- matrix(as.numeric(control_data$sensor.value), nrow = 256, ncol = 8)

# Smooth the matrix to obtain a functional data object for controls
control_fd <- smooth.basis(time, control_fd_matrix, fdParObj)$fd

# Perform FPCA for alcoholics (FP1 sensor)
res_alc <- pca.fd(alc_fd, nharm = nharms_95)
plot(res_alc)

# Perform FPCA for controls (FP1 sensor)
res_control <- pca.fd(control_fd, nharm = nharms_95)
plot(res_control)


```


```{r}
# Extract FP1 ordering data at time 0 for modeling
fp1_order <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr:: select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  filter(time == 0) %>%
  arrange(matching.condition, subject.identifier, name)

# Extract the full FP1 time series data
fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
 dplyr::  select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

```


```{r}
# Convert FP1 data to a matrix (256 x 48) and smooth it
pca_matrix_fp1 <- matrix(as.numeric(fp1_data$sensor.value), nrow = 256, ncol = 48)
pca_fd_fp1 <- smooth.basis(time, pca_matrix_fp1, fdParObj)$fd
plot(pca_fd_fp1)
# Apply FPCA to extract principal components
res_fp1 <- pca.fd(pca_fd_fp1, nharm = nbasis)  # Extracting the maximum possible components

# Determine the number of FPCs that explain at least 95% of the variance
nharms_95 <- which(cumsum(res_fp1$varprop) >= 0.95)[1]


# Run FPCA to extract 5 principal components
resfp1 <- pca.fd(pca_fd_fp1, nharm = nharms_95)
plot(resfp1)

# Extract FPCA scores and prepare modeling data
scores <- data.frame(resfp1$scores)
colnames(scores) <- paste0("FP1.", 1:9)

# Create a factor for the condition based on matching.condition
fp1_order$condition_factor <- factor(fp1_order$matching.condition)

# Combine ordering information with FPCA scores
fp1modeldata <- cbind(fp1_order, scores)

# # Build a multinomial logistic regression model using FP1 scores
# model_fp1 <- nnet:: multinom(condition_factor ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9, 
#                       data = fp1modeldata)
# summary(model_fp1)

```


```{r}
# Extract FP2 data
fp2_data <- firsttrial %>%
  filter(sensor.position == "FP2") %>%
  dplyr:: select(name, matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

# Convert FP2 sensor data into a matrix (256 x 48) and smooth it
pca_matrix_fp2 <- matrix(as.numeric(fp2_data$sensor.value), nrow = 256, ncol = 48)
pca_fd_fp2 <- smooth.basis(time, pca_matrix_fp2, fdParObj)$fd

# Run FPCA on FP2 data to extract 5 principal components
resfp2 <- pca.fd(pca_fd_fp2, nharm = nharms_95)
plot(resfp2)

# Extract FP2 scores
scores2 <- data.frame(resfp2$scores)
colnames(scores2) <- paste0("FP2.", 1:9)

```


```{r}
# Combine FP1 ordering data with FP1 and FP2 FPCA scores
fp1modeldata <- cbind(fp1_order, scores, scores2)
```


```{r,eval=F}
# Build a multinom model predicting subject identifier using combined scores
model_combined <- nnet:: multinom(matching.condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 +
                             FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, 
                           data = fp1modeldata)
summary(model_combined)

```

```{r,eval=F}
## Extract FP1 sensor data at time 0, arrange by subject and condition for modeling
 
fp1_order <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  filter(time==0) %>%
  arrange(matching.condition, subject.identifier, name)

fp1_data <- firsttrial %>%
  filter(sensor.position == "FP1") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

## Convert the FP1 time series data into a matrix form and smooth it to get the functional data object
pca_matrix <- matrix(as.numeric(fp1_data$sensor.value), 
                     nrow = 256, ncol = 48)

## Apply FPCA to extract 5 principal components and plot the FPCA results
pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd
resfp1 <- pca.fd(pca_fd, nharm = nharms_95)

plot(resfp1)


scores <- data.frame(resfp1$scores)
colnames(scores) <- c("FP1.1", "FP1.2", "FP1.3", "FP1.4", "FP1.5","FP1.6","FP1.7","FP1.8","FP1.9")
fp1_order$condition_factor <- factor(fp1_order$matching.condition)
#condition <- c(rep("S1 obj", 16), rep("S2 match", 16), rep("S2 No match,", 16))
#condition_factor <- factor(condition, levels = c("S1 obj", "S2 match", "S2 No match,"))
fp1modeldata = cbind(fp1_order,scores)

model <- nnet:: multinom(condition_factor ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9, data = fp1modeldata)


summary(model)
```

Building a Multinomial Model with FP1 and FP2 Data

```{r,eval=F}
fp2_data <- firsttrial %>%
  filter(sensor.position == "FP2") %>%
  dplyr::select(name,matching.condition, subject.identifier, time, sensor.value) %>%
  arrange(matching.condition, subject.identifier, name, time)

## Convert the FP2 data into a matrix and smooth it to create a functional data object
pca_matrix <- matrix(as.numeric(fp2_data$sensor.value), 
                     nrow = 256, ncol = 48)


pca_fd <- smooth.basis(time, pca_matrix, fdParObj)$fd
resfp2 <- pca.fd(pca_fd, nharm = nharms_95)

plot(resfp2)


scores2 <- data.frame(resfp2$scores)
colnames(scores2) <- c("FP2.1", "FP2.2", "FP2.3", "FP2.4", "FP2.5", "FP2.6", "FP2.7","FP2.8","FP2.9")

fp1modeldata = cbind(fp1_order,scores, scores2)

## Build a multinomial logistic regression model to predict subject identifier using combined FP1 & FP2 scores
model <- nnet:: multinom(matching.condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, data = fp1modeldata)


summary(model)
```

### Train-Test Split and Model Evaluation

```{r}
set.seed(123)  
 # clustering functions kmeans etc

## Partition the data into 75% training and 25% testing based on matching.condition
train_indices <- createDataPartition(fp1modeldata$matching.condition, p =0.75, list=F) 

as.vector(train_indices)

train_scores <- fp1modeldata[train_indices, ]
test_scores <- fp1modeldata[-train_indices, ]
train_condition <- train_scores$condition_factor
test_condition <- test_scores$condition_factor



train_scores_df <- data.frame(train_scores)
test_scores_df <- data.frame(test_scores)
```


```{r}
## Partition the data into 75% training and 25% testing based on matching.condition
train_model_fp1 <- nnet:: multinom(train_condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 , data = train_scores_df)

## Generate predictions on the test set and create a confusion matrix to evaluate performance
predictions <- predict(train_model_fp1, newdata = test_scores_df)


confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)


## Calculate and print the model's prediction accuracy
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)


```

Multinomial Model Using FP2 Scores Only

```{r}

## Train a multinomial logistic regression model using only the FP2 principal component scores.
train_model_fp2 <- nnet:: multinom(train_condition ~ FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, data = train_scores_df)


predictions <- predict(train_model_fp2, newdata = test_scores_df)

## Create a confusion matrix to compare predictions to actual test conditions.
confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)

## Calculate and print the accuracy of the FP2 model on the test set.
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)
```


```{r,eval=F}
## Partition the data based on matching condition (via condition_factor) into training and test sets.
train_indices <- createDataPartition(fp1modeldata$matching.condition, p =0.75, list=F) 

as.vector(train_indices)

train_scores <- fp1modeldata[train_indices, ]
test_scores <- fp1modeldata[-train_indices, ]
train_condition <- train_scores$condition_factor
test_condition <- test_scores$condition_factor



train_scores_df <- data.frame(train_scores)
test_scores_df <- data.frame(test_scores)

## Train a multinomial logistic regression model using FP1 principal component scores.
train_model_fp1 <- nnet:: multinom(train_condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 , data = train_scores_df)

## Predict matching.conditions on the test dataset using the FP1 model.
predictions <- predict(train_model_fp1, newdata = test_scores_df)

## Build a confusion matrix to compare predicted vs. actual matching.conditions.
confusion_matrix <- table(predictions, test_condition)
print(confusion_matrix)

## Calculate and print the model accuracy for the FP1-based predictions
accuracy <- sum(predictions == test_condition) / length(test_condition)
print(accuracy)

```

```{r,eval=F}
set.seed(123)  

# Partition the data into 75% training and 25% testing with stratification
train_indices <- createDataPartition(fp1modeldata$matching.condition, p = 0.75, list = FALSE)

# Create training and testing datasets
train_scores <- fp1modeldata[train_indices, ]
test_scores <- fp1modeldata[-train_indices, ]

# Extract labels
train_condition <- train_scores$condition_factor
test_condition <- test_scores$condition_factor

# Convert to data frames
train_scores_df <- data.frame(train_scores)
test_scores_df <- data.frame(test_scores)
```


# Train multinomial logistic regression model using FP1 & FP2 scores
```{r}
train_model_combined <- nnet::multinom(matching.condition ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 +
                                       FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, 
                                       data = train_scores_df, maxit = 1000)

# Predict on test data
predictions_combined <- predict(train_model_combined, newdata = test_scores_df)

# Confusion matrix & accuracy
conf_matrix_combined <- table(predictions_combined, test_condition)
print(conf_matrix_combined)

accuracy_combined <- sum(predictions_combined == test_condition) / length(test_condition)
print(paste("Multinomial Model Accuracy:", round(accuracy_combined * 100, 2), "%"))
```

# Random forest

```{r}
# Load required package
library(randomForest)

# Train Random Forest model
rf_model <- randomForest(as.factor(matching.condition) ~ FP1.1 + FP1.2 + FP1.3 + FP1.4 + FP1.5 + FP1.6 + FP1.7 + FP1.8 + FP1.9 +
                                                   FP2.1 + FP2.2 + FP2.3 + FP2.4 + FP2.5 + FP2.6 + FP2.7 + FP2.8 + FP2.9, 
                         data = train_scores_df, ntree = 500, importance = TRUE)

# Predict on test data
rf_predictions <- predict(rf_model, newdata = test_scores_df)

# Confusion matrix & accuracy
rf_conf_matrix <- table(rf_predictions, test_condition)
print(rf_conf_matrix)

rf_accuracy <- sum(rf_predictions == test_condition) / length(test_condition)
print(paste("Random Forest Model Accuracy:", round(rf_accuracy * 100, 2), "%"))

# Show variable importance from Random Forest
importance(rf_model)

```

```{r}
# Selecting only the most important FP1 & FP2 components
important_features <- c("FP1.4", "FP1.7", "FP1.9", "FP2.3", "FP2.8")

# Train multinomial model again
train_model_selected <- nnet::multinom(matching.condition ~ ., data = train_scores_df[, c("matching.condition", important_features)], maxit = 1000)

# Predict again
predictions_selected <- predict(train_model_selected, newdata = test_scores_df[, important_features])

# Confusion matrix & accuracy
conf_matrix_selected <- table(predictions_selected, test_condition)
print(conf_matrix_selected)

accuracy_selected <- sum(predictions_selected == test_condition) / length(test_condition)
print(paste("Multinomial Model Accuracy (Selected Features):", round(accuracy_selected * 100, 2), "%"))

```


```{r}
set.seed(123)
# train_indices <- createDataPartition(fp1modeldata$matching.condition, p = 0.75, list = FALSE)
# train_data <- fp1modeldata[train_indices, ]
# test_data <- fp1modeldata[-train_indices, ]

# Convert response variable to numeric for glmnet
y_train <- as.numeric(as.factor(train_scores$matching.condition)) - 1
y_test <- as.numeric(as.factor(test_scores$matching.condition)) - 1

# Prepare predictor matrices
X_train <- as.matrix(train_scores %>% dplyr::select(starts_with("FP")))
X_test <- as.matrix(test_scores %>% dplyr::select(starts_with("FP")))

# Standardize predictors
X_train <- scale(X_train)
X_test <- scale(X_test)

lasso_model <- cv.glmnet(X_train, y_train, alpha = 1, family = "multinomial")

# Predict on test set
lasso_pred <- predict(lasso_model, X_test, s = "lambda.min", type = "class")

# Convert predictions back to factors
lasso_pred <- factor(as.numeric(lasso_pred), levels = unique(y_train), labels = levels(train_scores$matching.condition))

# Compute accuracy
accuracy_lasso <- mean(lasso_pred == test_scores$matching.condition)
print(paste("LASSO Model Accuracy:", round(accuracy_lasso * 100, 2), "%"))
```
